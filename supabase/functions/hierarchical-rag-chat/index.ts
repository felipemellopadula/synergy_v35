import "https://deno.land/x/xhr@0.3.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";

const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Headers": "authorization, x-client-info, apikey, content-type",
};

// ============= CONFIGURA√á√ïES =============
const ACTIVATION_THRESHOLD_PAGES = 20;
const TARGET_OUTPUT_RATIO = 0.7; // 70% do original
const CHUNK_OUTPUT_RATIO = 0.9;  // N√≠vel 1: 90%
const SECTION_OUTPUT_RATIO = 0.8; // N√≠vel 2: 80%
const FINAL_OUTPUT_RATIO = 0.93; // N√≠vel 3: 93% (para atingir 70% total)

const TOKENS_PER_PAGE = 1400;
const CHARS_PER_PAGE = 3500;

// Configura√ß√µes de chunking adaptativo
const getChunkConfig = (pages: number) => {
  if (pages <= 50) return { chunkPages: 12, overlapPages: 2 };
  if (pages <= 100) return { chunkPages: 15, overlapPages: 3 };
  if (pages <= 200) return { chunkPages: 20, overlapPages: 4 };
  if (pages <= 500) return { chunkPages: 25, overlapPages: 5 };
  if (pages <= 1000) return { chunkPages: 30, overlapPages: 6 };
  return { chunkPages: 40, overlapPages: 8 };
};

// Estima√ß√£o de tokens
const estimateTokens = (text: string): number => Math.ceil(text.length / 4);

// Delay helper
const delay = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

// ============= N√çVEL 0: CHUNKING INTELIGENTE =============
const createAdaptiveChunks = (content: string, totalPages: number): string[] => {
  const { chunkPages, overlapPages } = getChunkConfig(totalPages);
  const chunkSize = chunkPages * CHARS_PER_PAGE;
  const overlapSize = overlapPages * CHARS_PER_PAGE;
  
  const chunks: string[] = [];
  let position = 0;
  
  while (position < content.length) {
    const end = Math.min(position + chunkSize, content.length);
    chunks.push(content.slice(position, end));
    position += (chunkSize - overlapSize);
    
    if (end === content.length) break;
  }
  
  console.log(`üìö Criados ${chunks.length} chunks (${chunkPages} p√°ginas cada, overlap ${overlapPages} p√°ginas)`);
  return chunks;
};

// ============= N√çVEL 1: CHUNK ANALYSIS =============
const analyzeChunk = async (
  chunk: string,
  chunkIndex: number,
  totalChunks: number,
  totalPages: number,
  openAIApiKey: string,
  retryCount = 0
): Promise<string> => {
  const chunkTokens = estimateTokens(chunk);
  const chunkPages = Math.ceil(chunkTokens / TOKENS_PER_PAGE);
  const targetOutputTokens = Math.floor(chunkTokens * CHUNK_OUTPUT_RATIO);
  
  console.log(`üîç Chunk ${chunkIndex + 1}/${totalChunks}: ${chunkPages} p√°ginas ‚Üí ${Math.floor(chunkPages * CHUNK_OUTPUT_RATIO)} p√°ginas`);
  
  const prompt = `Voc√™ √© um analista especializado em PRESERVA√á√ÉO M√ÅXIMA DE INFORMA√á√ÉO.

üìÑ CHUNK [${chunkIndex + 1} de ${totalChunks}] de um documento de ${totalPages} p√°ginas

${chunk}

üéØ MISS√ÉO: Crie uma an√°lise ULTRA-DETALHADA de ${targetOutputTokens} tokens (90% do original) que preserve:

1. üìã ESTRUTURA COMPLETA
   - Todos os t√≠tulos, subt√≠tulos e hierarquia
   - Numera√ß√£o de se√ß√µes e refer√™ncias
   - Organiza√ß√£o l√≥gica do conte√∫do

2. üíé CONTE√öDO ESSENCIAL (M√ÅXIMA PRESERVA√á√ÉO)
   - Todos os conceitos principais explicados em detalhes
   - Argumentos centrais com contexto completo
   - Defini√ß√µes e terminologias importantes
   - Exemplos relevantes e casos pr√°ticos

3. üìä DADOS CR√çTICOS (100% DE RETEN√á√ÉO)
   - Todas as tabelas, gr√°ficos e estat√≠sticas
   - N√∫meros, percentuais e m√©tricas exatas
   - Cita√ß√µes textuais relevantes
   - Refer√™ncias bibliogr√°ficas e fontes

4. üîó CONEX√ïES E RELA√á√ïES
   - Refer√™ncias a outras se√ß√µes do documento
   - Liga√ß√µes conceituais entre t√≥picos
   - Depend√™ncias e pr√©-requisitos

5. üß† INSIGHTS E AN√ÅLISE
   - Pontos de destaque e descobertas
   - Implica√ß√µes e consequ√™ncias
   - Quest√µes emergentes

‚ö†Ô∏è REGRAS CR√çTICAS:
- N√ÉO resuma excessivamente - mantenha riqueza de detalhes
- N√ÉO descarte informa√ß√µes secund√°rias
- MANTENHA o n√≠vel t√©cnico original
- USE Markdown (H2/H3/H4, listas, tabelas)
- PRESERVE cita√ß√µes literais

üéØ Target: ${targetOutputTokens} tokens (‚âà${Math.floor(chunkPages * CHUNK_OUTPUT_RATIO)} p√°ginas)`;

  try {
    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${openAIApiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "gpt-4.1-2025-04-14",
        messages: [{ role: "user", content: prompt }],
        max_completion_tokens: Math.min(64000, targetOutputTokens),
        temperature: 0.3,
        stream: false,
      }),
    });

    if (response.status === 429 && retryCount < 3) {
      console.log(`‚è≥ Rate limit hit, retrying chunk ${chunkIndex + 1} in 60s (attempt ${retryCount + 1}/3)`);
      await delay(60000);
      return analyzeChunk(chunk, chunkIndex, totalChunks, totalPages, openAIApiKey, retryCount + 1);
    }

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Chunk analysis failed: ${response.status} - ${errorText}`);
    }
    
    const data = await response.json();
    const result = data.choices[0].message.content;
    
    console.log(`‚úÖ Chunk ${chunkIndex + 1}: ${estimateTokens(result)} tokens gerados`);
    return result;
  } catch (error) {
    console.error(`‚ùå Error analyzing chunk ${chunkIndex + 1}:`, error);
    throw error;
  }
};

// ============= N√çVEL 2: SECTION SYNTHESIS =============
const synthesizeSection = async (
  chunkAnalyses: string[],
  sectionIndex: number,
  totalSections: number,
  openAIApiKey: string,
  retryCount = 0
): Promise<string> => {
  const totalSectionTokens = chunkAnalyses.reduce((sum, analysis) => sum + estimateTokens(analysis), 0);
  const targetOutputTokens = Math.floor(totalSectionTokens * SECTION_OUTPUT_RATIO);
  const totalSectionPages = Math.ceil(totalSectionTokens / TOKENS_PER_PAGE);
  
  console.log(`üß© Section ${sectionIndex + 1}/${totalSections}: ${chunkAnalyses.length} chunks (${totalSectionPages} p√°ginas) ‚Üí ${Math.floor(totalSectionPages * SECTION_OUTPUT_RATIO)} p√°ginas`);
  
  const prompt = `Voc√™ √© um sintetizador especializado em CONSOLIDA√á√ÉO SEM PERDA.

üìö SE√á√ÉO [${sectionIndex + 1} de ${totalSections}] - Consolidando ${chunkAnalyses.length} chunks

AN√ÅLISES DOS CHUNKS:
${chunkAnalyses.map((analysis, i) => `\n[CHUNK ${i+1}/${chunkAnalyses.length}]\n${analysis}\n`).join('\n---\n')}

üéØ MISS√ÉO: Crie uma S√çNTESE CONSOLIDADA de ${targetOutputTokens} tokens (80% do agregado) que:

1. üîó INTEGRE todos os chunks mantendo estrutura hier√°rquica e fluxo l√≥gico
2. üíé PRESERVE conceitos, argumentos, dados, tabelas e exemplos
3. üß© ELIMINE apenas redund√¢ncias e repeti√ß√µes entre chunks
4. ‚ú® ADICIONE conex√µes identificadas entre chunks e padr√µes recorrentes

‚ö†Ô∏è REGRAS:
- M√°xima fidelidade ao original
- N√ÉO crie conte√∫do novo
- PRESERVE dados num√©ricos e cita√ß√µes
- USE Markdown estruturado

üéØ Target: ${targetOutputTokens} tokens (‚âà${Math.floor(totalSectionPages * SECTION_OUTPUT_RATIO)} p√°ginas)`;

  try {
    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${openAIApiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "gpt-4.1-2025-04-14",
        messages: [{ role: "user", content: prompt }],
        max_completion_tokens: Math.min(64000, targetOutputTokens),
        temperature: 0.2,
        stream: false,
      }),
    });

    if (response.status === 429 && retryCount < 3) {
      console.log(`‚è≥ Rate limit hit, retrying section ${sectionIndex + 1} in 60s (attempt ${retryCount + 1}/3)`);
      await delay(60000);
      return synthesizeSection(chunkAnalyses, sectionIndex, totalSections, openAIApiKey, retryCount + 1);
    }

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Section synthesis failed: ${response.status} - ${errorText}`);
    }
    
    const data = await response.json();
    const result = data.choices[0].message.content;
    
    console.log(`‚úÖ Section ${sectionIndex + 1}: ${estimateTokens(result)} tokens gerados`);
    return result;
  } catch (error) {
    console.error(`‚ùå Error synthesizing section ${sectionIndex + 1}:`, error);
    throw error;
  }
};

// ============= TRANSFORMAR STREAM OPENAI ‚Üí SSE =============
const transformOpenAIStreamToSSE = (openAIStream: ReadableStream): ReadableStream => {
  const reader = openAIStream.getReader();
  const decoder = new TextDecoder();
  const encoder = new TextEncoder();
  
  return new ReadableStream({
    async start(controller) {
      let buffer = '';
      
      try {
        while (true) {
          const { done, value } = await reader.read();
          
          if (done) {
            console.log('‚úÖ Stream OpenAI conclu√≠do');
            controller.enqueue(encoder.encode('data: [DONE]\n\n'));
            controller.close();
            break;
          }
          
          buffer += decoder.decode(value, { stream: true });
          const lines = buffer.split('\n');
          buffer = lines.pop() || '';
          
          for (const line of lines) {
            const trimmed = line.trim();
            if (!trimmed || trimmed.startsWith(':')) continue;
            
            if (trimmed.startsWith('data: ')) {
              const data = trimmed.slice(6);
              
              if (data === '[DONE]') {
                console.log('üèÅ Recebido [DONE] da OpenAI');
                controller.enqueue(encoder.encode('data: [DONE]\n\n'));
                continue;
              }
              
              try {
                const parsed = JSON.parse(data);
                const content = parsed.choices?.[0]?.delta?.content;
                
                if (content) {
                  const sseEvent = `data: ${JSON.stringify({ content })}\n\n`;
                  controller.enqueue(encoder.encode(sseEvent));
                }
              } catch (e) {
                console.warn('‚ö†Ô∏è Erro ao parsear JSON da OpenAI:', e);
              }
            }
          }
        }
      } catch (error) {
        console.error('‚ùå Erro no transformStream:', error);
        controller.enqueue(encoder.encode(`data: ${JSON.stringify({ error: 'Stream error' })}\n\n`));
        controller.enqueue(encoder.encode('data: [DONE]\n\n'));
        controller.close();
      }
    }
  });
};

// ============= N√çVEL 3: DOCUMENT CONSOLIDATION =============
const consolidateDocument = async (
  sectionSyntheses: string[],
  userMessage: string,
  fileName: string,
  totalPages: number,
  openAIApiKey: string
): Promise<ReadableStream> => {
  const targetOutputTokens = Math.floor(totalPages * TOKENS_PER_PAGE * TARGET_OUTPUT_RATIO);
  const targetPages = Math.floor(totalPages * TARGET_OUTPUT_RATIO);
  
  console.log(`üéØ Consolida√ß√£o Final: ${totalPages} p√°ginas ‚Üí ${targetPages} p√°ginas (${targetOutputTokens} tokens)`);
  
  const prompt = `Voc√™ √© um especialista em AN√ÅLISE DOCUMENTAL PROFUNDA E CONSOLIDA√á√ÉO FINAL.

üìñ DOCUMENTO COMPLETO: "${fileName}" (${totalPages} p√°ginas)

S√çNTESES DAS SE√á√ïES:
${sectionSyntheses.map((synthesis, i) => `\n[SE√á√ÉO ${i+1}/${sectionSyntheses.length}]\n${synthesis}\n`).join('\n---\n')}

PERGUNTA/CONTEXTO DO USU√ÅRIO:
${userMessage}

üéØ MISS√ÉO: Crie uma AN√ÅLISE FINAL de ${targetOutputTokens} tokens (70% do original, ‚âà${targetPages} p√°ginas) com:

1. üåç PANORAMA GERAL: Vis√£o hol√≠stica, estrutura, objetivos
2. üìã CONTE√öDO CONSOLIDADO: Todos os t√≥picos com detalhes, conceitos, dados, exemplos
3. üî¨ AN√ÅLISE PROFUNDA: Padr√µes globais, conex√µes, insights, avalia√ß√£o cr√≠tica
4. üìä DADOS ESTRUTURADOS: Tabelas, listas, estat√≠sticas, refer√™ncias
5. üéØ RESPOSTA DIRETA: Resposta √† pergunta do usu√°rio, recomenda√ß√µes
6. üí° INSIGHTS: Takeaways, implica√ß√µes, pr√≥ximos passos

‚ö†Ô∏è REGRAS:
- MANTENHA 70% do conte√∫do (${targetPages} p√°ginas)
- N√ÉO resuma excessivamente
- USE Markdown extensivamente
- PRESERVE fidelidade m√°xima

üéØ Target: ${targetOutputTokens} tokens (${targetPages} p√°ginas)`;

  try {
    console.log('üì° Chamando OpenAI para consolida√ß√£o final...');
    
    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${openAIApiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "gpt-4.1-2025-04-14",
        messages: [{ role: "user", content: prompt }],
        max_completion_tokens: Math.min(64000, targetOutputTokens),
        temperature: 0.2,
        stream: true,
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error(`‚ùå OpenAI retornou erro ${response.status}:`, errorText);
      throw new Error(`Document consolidation failed: ${response.status} - ${errorText}`);
    }
    
    if (!response.body) {
      throw new Error('OpenAI response body is null');
    }
    
    console.log('üîÑ Transformando stream OpenAI ‚Üí SSE');
    return transformOpenAIStreamToSSE(response.body);
    
  } catch (error) {
    console.error('‚ùå Erro na consolida√ß√£o do documento:', error);
    throw error;
  }
};

// ============= PROCESSAMENTO PARALELO =============
const processChunksInParallel = async (
  chunks: string[],
  totalPages: number,
  openAIApiKey: string
): Promise<string[]> => {
  const batchSize = 5;
  const results: string[] = [];
  
  for (let i = 0; i < chunks.length; i += batchSize) {
    const batch = chunks.slice(i, Math.min(i + batchSize, chunks.length));
    console.log(`üì¶ Processando batch ${Math.floor(i/batchSize) + 1}/${Math.ceil(chunks.length/batchSize)}`);
    
    const batchPromises = batch.map((chunk, idx) => 
      analyzeChunk(chunk, i + idx, chunks.length, totalPages, openAIApiKey)
    );
    
    const batchResults = await Promise.allSettled(batchPromises);
    
    batchResults.forEach((result, idx) => {
      if (result.status === 'fulfilled') {
        results.push(result.value);
      } else {
        console.error(`‚ùå Chunk ${i + idx + 1} falhou:`, result.reason);
        throw new Error(`Chunk processing failed: ${result.reason}`);
      }
    });
    
    // Rate limiting
    if (i + batchSize < chunks.length) {
      await delay(2000);
    }
  }
  
  return results;
};

// ============= AGRUPAMENTO EM SE√á√ïES =============
const groupIntoSections = (chunkAnalyses: string[], totalPages: number): string[][] => {
  let sectionsCount: number;
  if (totalPages <= 50) sectionsCount = 1;
  else if (totalPages <= 100) sectionsCount = 3;
  else if (totalPages <= 200) sectionsCount = 5;
  else if (totalPages <= 500) sectionsCount = 8;
  else sectionsCount = 12;
  
  const chunksPerSection = Math.ceil(chunkAnalyses.length / sectionsCount);
  const sections: string[][] = [];
  
  for (let i = 0; i < chunkAnalyses.length; i += chunksPerSection) {
    sections.push(chunkAnalyses.slice(i, i + chunksPerSection));
  }
  
  console.log(`üìÇ Agrupados ${chunkAnalyses.length} chunks em ${sections.length} se√ß√µes`);
  return sections;
};

// ============= SERVIDOR PRINCIPAL =============
serve(async (req) => {
  if (req.method === "OPTIONS") {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { message, documentContent, pageCount, fileName } = await req.json();
    
    console.log(`üöÄ Hierarchical RAG ativado: ${fileName} (${pageCount} p√°ginas)`);
    console.log(`üéØ Target output: ${Math.floor(pageCount * TARGET_OUTPUT_RATIO)} p√°ginas (70% do original)`);
    
    const openAIApiKey = Deno.env.get("OPENAI_API_KEY");
    if (!openAIApiKey) throw new Error("OPENAI_API_KEY not configured");
    
    // N√çVEL 1: Chunk Analysis
    console.log("\nüìä N√çVEL 1: Chunk Analysis");
    const chunks = createAdaptiveChunks(documentContent, pageCount);
    const chunkAnalyses = await processChunksInParallel(chunks, pageCount, openAIApiKey);
    
    // N√çVEL 2: Section Synthesis
    console.log("\nüß© N√çVEL 2: Section Synthesis");
    const sections = groupIntoSections(chunkAnalyses, pageCount);
    const sectionPromises = sections.map((section, idx) => 
      synthesizeSection(section, idx, sections.length, openAIApiKey)
    );
    const sectionSyntheses = await Promise.all(sectionPromises);
    
    // N√çVEL 3: Document Consolidation + Streaming
    console.log("\nüéØ N√çVEL 3: Document Consolidation (streaming)");
    const finalStream = await consolidateDocument(
      sectionSyntheses,
      message,
      fileName,
      pageCount,
      openAIApiKey
    );
    
    return new Response(finalStream, {
      headers: {
        ...corsHeaders,
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
      },
    });
    
  } catch (error: any) {
    console.error("‚ùå Hierarchical RAG error:", error);
    return new Response(
      JSON.stringify({ error: error.message || "Processing failed" }),
      {
        status: 500,
        headers: { ...corsHeaders, "Content-Type": "application/json" },
      }
    );
  }
});
