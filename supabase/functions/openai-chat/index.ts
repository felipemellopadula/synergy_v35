import "https://deno.land/x/xhr@0.3.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";

const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Headers": "authorization, x-client-info, apikey, content-type",
};

// Mapeamento de IDs de modelos para nomes oficiais da OpenAI API
const mapModelToOpenAI = (model: string): string => {
  const modelMap: Record<string, string> = {
    'gpt-5.1': 'gpt-5',
    'gpt-5-mini': 'gpt-5-mini',
    'gpt-5-nano': 'gpt-5-nano',
    'gpt-4.1': 'gpt-4.1',
    'gpt-4.1-mini': 'gpt-4.1-mini',
    'gpt-4.1-nano': 'gpt-4.1-nano',
    'o4-mini': 'o4-mini',
    'o3': 'o3',
    'gpt-4o-mini': 'gpt-4o-mini',
    'gpt-4o': 'gpt-4o',
  };
  
  const mapped = modelMap[model] || model;
  console.log(`üîÑ Model mapping: ${model} ‚Üí ${mapped}`);
  return mapped;
};

// Fun√ß√£o auxiliar para estimar tokens
const estimateTokens = (text: string): number => {
  return Math.ceil(text.length / 4);
};

// ‚úÖ TIER-2-MAXOUT: Limites de output por modelo (max_completion_tokens)
const getModelOutputLimits = (model: string): number => {
  // GPT-5 family - m√°ximo output de 128k
  if (model.includes('gpt-5') && !model.includes('mini') && !model.includes('nano')) return 128000;
  
  // GPT-5 Mini - m√°ximo output de 32k
  if (model.includes('gpt-5-mini')) return 32768;
  
  // GPT-5 Nano - m√°ximo output de 16k
  if (model.includes('gpt-5-nano')) return 16384;
  
  // GPT-4.1 family - m√°ximo output de 32k
  if (model.includes('gpt-4.1')) return 32768;
  
  // o3 e o4-mini - m√°ximo output de 100k
  if (model.includes('o3') || model.includes('o4-mini')) return 100000;
  
  // GPT-4o family (legacy) - m√°ximo output de 16k
  if (model.includes('gpt-4o')) return 16384;
  
  return 16384; // Default fallback
};

// ‚úÖ TIER-2-MAXOUT: Limites de input/context por modelo
const getModelInputLimits = (model: string): number => {
  if (model.includes('gpt-5') && !model.includes('mini') && !model.includes('nano')) return 400000;
  if (model.includes('gpt-5-mini')) return 200000;
  if (model.includes('gpt-5-nano')) return 100000;
  if (model.includes('gpt-4.1')) return 1047576; // 1M+ tokens!
  if (model.includes('o3') || model.includes('o4-mini')) return 200000;
  if (model.includes('gpt-4o')) return 128000;
  return 128000;
};

// ‚úÖ TIER-2-MAXOUT: Threshold din√¢mico baseado no contexto do modelo
const getMapReduceThreshold = (model: string): number => {
  // GPT-4.1 tem contexto de 1M tokens - threshold alt√≠ssimo
  if (model.includes('gpt-4.1')) return 500000;
  
  // GPT-5 tem contexto de 400k - threshold alto
  if (model.includes('gpt-5') && !model.includes('mini') && !model.includes('nano')) return 200000;
  
  // GPT-5 Mini/o3/o4-mini - contexto de 200k
  if (model.includes('gpt-5-mini') || model.includes('o3') || model.includes('o4')) return 100000;
  
  // GPT-4o family
  if (model.includes('gpt-4o')) return 60000;
  
  return 50000;
};

// ‚úÖ TIER-2-MAXOUT: Chunk size din√¢mico baseado no modelo
const getChunkSize = (model: string): number => {
  if (model.includes('gpt-4.1')) return 200000; // Chunks enormes para 1M context
  if (model.includes('gpt-5') && !model.includes('mini') && !model.includes('nano')) return 100000;
  if (model.includes('gpt-5-mini') || model.includes('o3') || model.includes('o4')) return 50000;
  return 20000;
};

// ‚úÖ TIER-2-MAXOUT: Fun√ß√£o para dividir texto em chunks inteligentes com tamanho din√¢mico
const chunkText = (text: string, model: string): string[] => {
  const maxChunkTokens = getChunkSize(model);
  const estimatedTokens = estimateTokens(text);
  const numChunks = Math.ceil(estimatedTokens / maxChunkTokens);
  
  if (numChunks <= 1) return [text];
  
  const chunkSize = Math.ceil(text.length / numChunks);
  const chunks: string[] = [];
  
  for (let i = 0; i < numChunks; i++) {
    const start = i * chunkSize;
    const end = Math.min(start + chunkSize, text.length);
    chunks.push(text.slice(start, end));
  }
  
  console.log(`üìö TIER-2-MAXOUT: Divided into ${chunks.length} chunks (${maxChunkTokens} tokens each, model: ${model})`);
  return chunks;
};

// Fun√ß√£o para processar um chunk individual
const processChunk = async (
  chunk: string,
  chunkIndex: number,
  totalChunks: number,
  model: string,
  openAIApiKey: string,
  retries = 1
): Promise<string> => {
  console.log(`üîç Processing chunk ${chunkIndex + 1}/${totalChunks}...`);
  
  const isNewerModel = model.includes("gpt-5") || model.includes("gpt-4.1") || model.includes("o3") || model.includes("o4");
  const apiModel = mapModelToOpenAI(model);
  
  const maxOutputLimit = getModelOutputLimits(model);
  const chunkOutputTokens = Math.min(maxOutputLimit, Math.floor(maxOutputLimit * 0.5)); // 50% do limite para chunks
  
  const requestBody: any = {
    model: apiModel,
    messages: [
      {
        role: "system",
        content: "Voc√™ √© um assistente especializado em an√°lise profunda de documentos. Extraia insights detalhados, padr√µes, dados-chave e conclus√µes relevantes."
      },
      {
        role: "user",
        content: `Analise esta parte (${chunkIndex + 1} de ${totalChunks}) do documento completo. Seja extremamente detalhado e profundo:\n\n${chunk}`
      }
    ],
    stream: false,
  };

  if (!isNewerModel) {
    requestBody.max_tokens = chunkOutputTokens;
    requestBody.temperature = 0.7;
  } else {
    requestBody.max_completion_tokens = chunkOutputTokens;
  }
  
  console.log(`üìù TIER-2-MAXOUT chunk ${chunkIndex + 1}: max_completion_tokens=${chunkOutputTokens} (limit: ${maxOutputLimit})`);

  for (let attempt = 0; attempt <= retries; attempt++) {
    try {
      const response = await fetch("https://api.openai.com/v1/chat/completions", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${openAIApiKey}`,
          "Content-Type": "application/json",
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        if (response.status === 429 && attempt < retries) {
          const delay = Math.pow(2, attempt) * 1000;
          console.log(`‚è±Ô∏è Rate limit hit, waiting ${delay}ms...`);
          await new Promise(resolve => setTimeout(resolve, delay));
          continue;
        }
        throw new Error(`OpenAI API error: ${response.status}`);
      }

      const data = await response.json();
      const content = data.choices[0].message.content;
      console.log(`‚úÖ Chunk ${chunkIndex + 1}/${totalChunks} processed (${estimateTokens(content)} tokens)`);
      return content;
    } catch (error) {
      if (attempt === retries) throw error;
      console.log(`‚ö†Ô∏è Retry ${attempt + 1}/${retries} for chunk ${chunkIndex + 1}`);
    }
  }
  
  throw new Error(`Failed to process chunk ${chunkIndex + 1} after ${retries} retries`);
};

// Fun√ß√£o para consolidar respostas e fazer streaming
const consolidateResponses = async (
  chunkResponses: string[],
  originalMessage: string,
  model: string,
  openAIApiKey: string
): Promise<ReadableStream> => {
  console.log(`üß† Consolidating ${chunkResponses.length} chunk responses...`);
  
  const consolidationPrompt = `Voc√™ analisou um documento longo dividido em ${chunkResponses.length} partes. Aqui est√£o as an√°lises de cada parte:

${chunkResponses.map((resp, i) => `\n[PARTE ${i + 1}/${chunkResponses.length}]\n${resp}\n`).join('\n---\n')}

Agora, crie uma resposta FINAL consolidada e profunda (12.000-16.000 palavras, aproximadamente 16 p√°ginas) que:
- Resume os pontos mais importantes de TODAS as partes
- Identifica padr√µes e conex√µes entre as diferentes se√ß√µes
- Fornece insights profundos e an√°lise cr√≠tica
- Organiza a informa√ß√£o de forma l√≥gica e bem estruturada
- Inclui exemplos e cita√ß√µes espec√≠ficas do documento
- Mantenha o formato em Markdown com t√≠tulos, subt√≠tulos e listas

Pergunta/contexto original do usu√°rio: ${originalMessage}`;

  const isNewerModel = model.includes("gpt-5") || model.includes("gpt-4.1") || model.includes("o3") || model.includes("o4");
  const apiModel = mapModelToOpenAI(model);
  
  const maxOutputLimit = getModelOutputLimits(model);
  const consolidationOutputTokens = Math.min(maxOutputLimit, Math.floor(maxOutputLimit * 0.8)); // 80% do limite para consolida√ß√£o final
  
  const requestBody: any = {
    model: apiModel,
    messages: [
      {
        role: "system",
        content: "Voc√™ √© um assistente especializado em criar an√°lises consolidadas extremamente detalhadas e profundas de documentos. Suas respostas devem ser extensas, bem estruturadas e ricas em insights."
      },
      {
        role: "user",
        content: consolidationPrompt
      }
    ],
    stream: true,
    store: true, // Enable prompt caching for consolidation
  };

  if (!isNewerModel) {
    requestBody.max_tokens = consolidationOutputTokens;
    requestBody.temperature = 0.7;
  } else {
    requestBody.max_completion_tokens = consolidationOutputTokens;
  }
  
  console.log(`üìù TIER-2-MAXOUT consolidation: max_completion_tokens=${consolidationOutputTokens} (limit: ${maxOutputLimit})`);

  const response = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      "Authorization": `Bearer ${openAIApiKey}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify(requestBody),
  });

  if (!response.ok) {
    throw new Error(`Consolidation failed: ${response.status}`);
  }

  return response.body!;
};

serve(async (req) => {
  if (req.method === "OPTIONS") {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { 
      message, 
      model = "gpt-5-mini", 
      files = [], 
      conversationHistory = [],
      webSearchEnabled = false 
    } = await req.json();
    
    console.log(`üîÑ Request for model: ${model}, History length: ${conversationHistory.length}, Web Search: ${webSearchEnabled}`);

    const openAIApiKey = Deno.env.get("OPENAI_API_KEY");
    if (!openAIApiKey) {
      throw new Error("OPENAI_API_KEY n√£o configurada");
    }

    // ============ WEB SEARCH MODE ============
    // Uses OpenAI Responses API with web_search tool
    if (webSearchEnabled) {
      console.log("üåê Web Search mode enabled - using Responses API");
      
      // Models that support web search
      const webSearchModels = ['gpt-5', 'gpt-5-mini', 'gpt-5-nano', 'gpt-4.1', 'o4-mini', 'o3'];
      const apiModel = mapModelToOpenAI(model);
      
      if (!webSearchModels.some(m => apiModel.includes(m))) {
        console.warn(`‚ö†Ô∏è Model ${apiModel} may not support web search, proceeding anyway`);
      }
      
      // Build conversation context for Responses API
      const conversationInput: any[] = [];
      
      // Add conversation history
      if (conversationHistory && conversationHistory.length > 0) {
        conversationHistory.slice(-6).forEach((msg: any) => {
          conversationInput.push({
            role: msg.role === 'user' ? 'user' : 'assistant',
            content: msg.content
          });
        });
      }
      
      // Add current message
      conversationInput.push({
        role: 'user',
        content: message
      });
      
      const responsesBody: any = {
        model: apiModel,
        input: conversationInput,
        tools: [
          { type: "web_search" }
        ],
        tool_choice: "auto",
        stream: true,
      };
      
      console.log(`üåê Sending web search request to Responses API with model: ${apiModel}`);
      
      const response = await fetch("https://api.openai.com/v1/responses", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${openAIApiKey}`,
          "Content-Type": "application/json",
        },
        body: JSON.stringify(responsesBody),
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error("‚ùå OpenAI Responses API error:", response.status, errorText);
        
        if (response.status === 429) {
          return new Response(
            JSON.stringify({ error: "Rate limit exceeded. Please try again later." }),
            { status: 429, headers: { ...corsHeaders, "Content-Type": "application/json" } }
          );
        }
        
        throw new Error(`OpenAI Responses API error: ${response.status} - ${errorText}`);
      }

      console.log("‚úÖ Streaming web search response from OpenAI");
      
      // Transform Responses API SSE to Chat Completions format for frontend compatibility
      const encoder = new TextEncoder();
      const transformedStream = new ReadableStream({
        async start(controller) {
          const reader = response.body!.getReader();
          const decoder = new TextDecoder();
          let buffer = '';
          let webSearchCalls: any[] = [];
          let outputText = '';

          try {
            while (true) {
              const { done, value } = await reader.read();
              if (done) break;

              buffer += decoder.decode(value, { stream: true });
              const lines = buffer.split('\n');
              buffer = lines.pop() || '';

              for (const line of lines) {
                if (!line.trim() || line.startsWith(':')) continue;
                if (!line.startsWith('data: ')) continue;

                const jsonStr = line.slice(6);
                if (jsonStr === '[DONE]') continue;

                try {
                  const event = JSON.parse(jsonStr);
                  
                  // Handle different Responses API event types
                  if (event.type === 'response.web_search_call.searching') {
                    // Web search in progress
                    const searchQuery = event.query || 'searching...';
                    controller.enqueue(encoder.encode(`data: ${JSON.stringify({
                      type: 'web_search_status',
                      status: `üîç Searching: "${searchQuery}"`
                    })}\n\n`));
                    console.log(`üîç Web search query: ${searchQuery}`);
                  }
                  
                  if (event.type === 'response.web_search_call.completed') {
                    webSearchCalls.push(event);
                    controller.enqueue(encoder.encode(`data: ${JSON.stringify({
                      type: 'web_search_status',
                      status: '‚úÖ Search completed, generating response...'
                    })}\n\n`));
                    console.log('‚úÖ Web search completed');
                  }
                  
                  // Handle content delta (text being streamed)
                  if (event.type === 'response.output_text.delta') {
                    const text = event.delta || '';
                    outputText += text;
                    // Convert to Chat Completions format
                    controller.enqueue(encoder.encode(`data: ${JSON.stringify({
                      choices: [{
                        delta: { content: text }
                      }]
                    })}\n\n`));
                  }
                  
                  // Alternative: content_part.delta for some response types
                  if (event.type === 'response.content_part.delta' && event.delta?.text) {
                    const text = event.delta.text;
                    outputText += text;
                    controller.enqueue(encoder.encode(`data: ${JSON.stringify({
                      choices: [{
                        delta: { content: text }
                      }]
                    })}\n\n`));
                  }
                  
                  // Handle completed response with annotations (citations)
                  if (event.type === 'response.output_text.done') {
                    const annotations = event.annotations || [];
                    if (annotations.length > 0) {
                      console.log(`üìö Found ${annotations.length} citations`);
                      // Send citations as metadata
                      controller.enqueue(encoder.encode(`data: ${JSON.stringify({
                        type: 'citations',
                        citations: annotations.map((a: any) => ({
                          url: a.url,
                          title: a.title,
                          startIndex: a.start_index,
                          endIndex: a.end_index
                        }))
                      })}\n\n`));
                    }
                  }
                  
                  if (event.type === 'response.done') {
                    console.log('üéâ Response complete');
                  }
                  
                } catch (e) {
                  // Ignore parse errors
                }
              }
            }

            controller.enqueue(encoder.encode('data: [DONE]\n\n'));
            controller.close();
            
            console.log(`üìä Web search response completed: ${outputText.length} chars, ${webSearchCalls.length} searches`);
            
          } catch (e) {
            console.error('Stream error:', e);
            controller.error(e);
          }
        }
      });

      return new Response(transformedStream, {
        headers: {
          ...corsHeaders,
          "Content-Type": "text/event-stream",
          "Cache-Control": "no-cache",
          "Connection": "keep-alive",
        },
      });
    }

    // ============ NORMAL CHAT MODE ============

    // Estimar tokens da mensagem
    const estimatedTokens = estimateTokens(message);
    console.log(`üìä Token estimation: ${estimatedTokens} tokens for model ${model}`);
    console.log(`üîç Document size: ${estimatedTokens} tokens (${Math.ceil(estimatedTokens / 400)} p√°ginas aprox.)`);

    // ‚úÖ TIER-2-MAXOUT-PLUS: Threshold din√¢mico baseado no modelo
    const threshold = getMapReduceThreshold(model);
    const needsMapReduce = estimatedTokens > threshold;
    console.log(`üìä Map-Reduce ${needsMapReduce ? 'ATIVADO ‚úÖ' : 'DESATIVADO ‚ùå'} (threshold: ${threshold} tokens, modelo: ${model})`);

    if (needsMapReduce) {
      console.log(`üóÇÔ∏è Large document detected (${estimatedTokens} tokens) - using Map-Reduce approach`);
      
      try {
        // MAP PHASE: Dividir em chunks e processar cada um
        const chunks = chunkText(message, model); // ‚úÖ TIER-2-MAXOUT: Chunk size din√¢mico baseado no modelo
        console.log(`üìö TIER-2-MAXOUT: Processing ${chunks.length} chunks in parallel...`);
        
        const chunkPromises = chunks.map((chunk, i) => 
          processChunk(chunk, i, chunks.length, model, openAIApiKey, 1)
        );
        
        const chunkResponses = await Promise.all(chunkPromises);
        console.log(`‚úÖ All ${chunks.length} chunks processed successfully`);
        
        // REDUCE PHASE: Consolidar respostas e fazer streaming
        const consolidatedStream = await consolidateResponses(
          chunkResponses,
          conversationHistory.length > 0 
            ? conversationHistory[conversationHistory.length - 1].content 
            : "Analise este documento",
          model,
          openAIApiKey
        );
        
        console.log("üß† Streaming consolidated response...");
        
        return new Response(consolidatedStream, {
          headers: {
            ...corsHeaders,
            "Content-Type": "text/event-stream",
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
          },
        });
      } catch (error: any) {
        console.error("‚ùå Map-Reduce error:", error);
        return new Response(
          JSON.stringify({ error: `Map-Reduce processing failed: ${error.message}` }),
          {
            status: 500,
            headers: { ...corsHeaders, "Content-Type": "application/json" },
          }
        );
      }
    }

    // STREAMING DIRETO para documentos menores
    console.log("üìù Document size OK - using direct streaming");
    
    // ‚úÖ PROMPT CACHING: OpenAI automaticamente cacheia prompts com 1024+ tokens
    // Estrat√©gia: System prompt primeiro + hist√≥rico completo para maximizar cache hits
    const messages: any[] = [
      {
        role: "system",
        content: "Voc√™ √© um assistente √∫til e preciso. Responda de forma clara e organizada. " +
                 "Analise cuidadosamente as mensagens anteriores para manter contexto e coer√™ncia nas respostas."
      },
    ];

    // ‚úÖ CACHE OPTIMIZATION: Adicionar TODO o hist√≥rico antes da mensagem atual
    // Isso permite que a OpenAI cacheia o contexto completo da conversa
    if (conversationHistory && conversationHistory.length > 0) {
      conversationHistory.forEach((msg: any) => {
        messages.push({
          role: msg.role,
          content: msg.content,
        });
      });
      console.log(`üíæ Cache-eligible context: ${conversationHistory.length} messages in history`);
    }

    // Processar arquivos (imagens para vis√£o)
    if (files && files.length > 0) {
      const imageFiles = files.filter((f: any) => f.type?.startsWith("image/"));
      
      if (imageFiles.length > 0) {
        // Modelo com vis√£o - formato especial
        const content: any[] = [{ type: "text", text: message }];
        
        imageFiles.forEach((file: any) => {
          if (file.imageData) {
            content.push({
              type: "image_url",
              image_url: {
                url: file.imageData,
              },
            });
          }
        });

        messages.push({
          role: "user",
          content,
        });
      } else {
        // Sem imagens - mensagem de texto simples
        messages.push({
          role: "user",
          content: message,
        });
      }
    } else {
      messages.push({
        role: "user",
        content: message,
      });
    }

    const apiModel = mapModelToOpenAI(model);
    console.log(`üöÄ Sending request to OpenAI with model: ${apiModel}`);

    // Determinar par√¢metros baseado no modelo
    const isNewerModel = model.includes("gpt-5") || model.includes("gpt-4.1") || model.includes("o3") || model.includes("o4");
    
    const requestBody: any = {
      model: apiModel,
      messages,
      stream: true,
      store: true, // Enable prompt caching (50% discount on cached tokens)
    };

    // ‚úÖ TIER-2-MAXOUT: Output din√¢mico inteligente baseado no modelo e input
    const maxOutputLimit = getModelOutputLimits(model);
    const maxInputLimit = getModelInputLimits(model);
    
    const maxOutputTokens = Math.min(
      maxOutputLimit,
      Math.max(
        Math.floor(maxOutputLimit * 0.5), // M√≠nimo 50% do limite do modelo
        maxOutputLimit - Math.floor(estimatedTokens * 0.1) // Reserva 10% para safety margin
      )
    );
    
    console.log(`üí° TIER-2-MAXOUT: Dynamic output=${maxOutputTokens} (limit: ${maxOutputLimit}, input: ${estimatedTokens}, context: ${maxInputLimit})`);

    // Apenas modelos antigos suportam max_tokens e temperature
    if (!isNewerModel) {
      requestBody.max_tokens = maxOutputTokens;
      requestBody.temperature = 0.7;
    } else {
      // Modelos novos usam max_completion_tokens
      requestBody.max_completion_tokens = maxOutputTokens;
    }
    
    console.log(`üìù Direct streaming config: max_completion_tokens=${requestBody.max_completion_tokens || requestBody.max_tokens}`);

    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${openAIApiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify(requestBody),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error("‚ùå OpenAI API error:", response.status, errorText);
      
      // Log cache-related errors if present
      try {
        const errorData = JSON.parse(errorText);
        if (errorData.error?.type === 'invalid_request_error' && errorData.error?.message?.includes('cache')) {
          console.error("‚ö†Ô∏è Cache-related error detected:", errorData.error.message);
        }
      } catch (e) {
        // Ignore JSON parse errors
      }
      
      if (response.status === 429) {
        return new Response(
          JSON.stringify({ error: "Rate limit exceeded. Please try again later." }),
          {
            status: 429,
            headers: { ...corsHeaders, "Content-Type": "application/json" },
          }
        );
      }
      
      if (response.status === 402) {
        return new Response(
          JSON.stringify({ error: "Insufficient credits. Please add funds to your OpenAI account." }),
          {
            status: 402,
            headers: { ...corsHeaders, "Content-Type": "application/json" },
          }
        );
      }

      throw new Error(`OpenAI API error: ${response.status} - ${errorText}`);
    }

    console.log("‚úÖ Streaming response from OpenAI");
    
    // üíæ PROMPT CACHING: Monitor cache status
    const cacheHeader = response.headers.get('openai-cache-status');
    if (cacheHeader) {
      console.log(`üíæ Prompt Cache Status: ${cacheHeader}`);
    }
    
    // Log cache-related headers for debugging
    const cacheReadTokens = response.headers.get('openai-cache-read-tokens');
    const cacheWriteTokens = response.headers.get('openai-cache-write-tokens');
    
    if (cacheReadTokens) {
      console.log(`üîÑ Cache Read Tokens: ${cacheReadTokens} (50% discount applied)`);
    }
    if (cacheWriteTokens) {
      console.log(`üìù Cache Write Tokens: ${cacheWriteTokens} (full price)`);
    }

    // Retornar stream SSE diretamente
    return new Response(response.body, {
      headers: {
        ...corsHeaders,
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
      },
    });

  } catch (error: any) {
    console.error("‚ùå Error in openai-chat:", error);
    return new Response(
      JSON.stringify({ error: error.message || "Unknown error occurred" }),
      {
        status: 500,
        headers: { ...corsHeaders, "Content-Type": "application/json" },
      }
    );
  }
});
