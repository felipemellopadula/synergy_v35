import "https://deno.land/x/xhr@0.3.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";

const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Headers": "authorization, x-client-info, apikey, content-type",
};

// Fun√ß√£o auxiliar para estimar tokens
const estimateTokens = (text: string): number => {
  return Math.ceil(text.length / 4);
};

// ‚úÖ TIER-2-MAXOUT-PLUS: Threshold din√¢mico baseado no modelo
const getMapReduceThreshold = (model: string): number => {
  if (model.includes('gpt-4o')) return 20000; // ~50 p√°ginas para modelos com contexto maior
  if (model.includes('gpt-4')) return 15000;  // ~40 p√°ginas (padr√£o)
  return 10000; // ~25 p√°ginas para modelos menores
};

// Mapeamento de nomes de modelo do frontend para nomes da API OpenAI
const mapModelName = (model: string): string => {
  const modelMap: Record<string, string> = {
    // GPT-5 Series
    'gpt-5.1': 'gpt-5-2025-08-07',
    'gpt-5-mini': 'gpt-5-mini-2025-08-07',
    'gpt-5-nano': 'gpt-5-nano-2025-08-07',
    
    // GPT-4.1 Series
    'gpt-4.1': 'gpt-4.1-2025-04-14',
    'gpt-4.1-mini': 'gpt-4.1-mini-2025-04-14',
    'gpt-4.1-nano': 'gpt-4.1-nano-2025-04-14',
    
    // O-Series
    'o3': 'o3-2025-04-16',
    'o4-mini': 'o4-mini-2025-04-16',
    
    // Legacy models (fallback)
    'gpt-4o-mini': 'gpt-4o-mini',
    'gpt-4o': 'gpt-4o',
  };
  
  const mappedModel = modelMap[model] || model;
  if (mappedModel !== model) {
    console.log(`üîÑ Model mapped: ${model} ‚Üí ${mappedModel}`);
  }
  return mappedModel;
};

// Fun√ß√£o para dividir texto em chunks inteligentes
const chunkText = (text: string, maxChunkTokens: number): string[] => {
  const estimatedTokens = estimateTokens(text);
  const numChunks = Math.ceil(estimatedTokens / maxChunkTokens);
  
  if (numChunks <= 1) return [text];
  
  const chunkSize = Math.ceil(text.length / numChunks);
  const chunks: string[] = [];
  
  for (let i = 0; i < numChunks; i++) {
    const start = i * chunkSize;
    const end = Math.min(start + chunkSize, text.length);
    chunks.push(text.slice(start, end));
  }
  
  console.log(`üìö Divided into ${chunks.length} chunks (avg ${Math.ceil(estimateTokens(chunks[0]))} tokens each)`);
  return chunks;
};

// Fun√ß√£o para processar um chunk individual
const processChunk = async (
  chunk: string,
  chunkIndex: number,
  totalChunks: number,
  model: string,
  openAIApiKey: string,
  retries = 1
): Promise<string> => {
  console.log(`üîç Processing chunk ${chunkIndex + 1}/${totalChunks}...`);
  
  const isNewerModel = model.includes("gpt-5") || model.includes("gpt-4.1") || model.includes("o3") || model.includes("o4");
  
  const requestBody: any = {
    model,
    messages: [
      {
        role: "system",
        content: "Voc√™ √© um assistente especializado em an√°lise profunda de documentos. Extraia insights detalhados, padr√µes, dados-chave e conclus√µes relevantes."
      },
      {
        role: "user",
        content: `Analise esta parte (${chunkIndex + 1} de ${totalChunks}) do documento completo. Seja extremamente detalhado e profundo:\n\n${chunk}`
      }
    ],
    stream: false,
  };

  if (!isNewerModel) {
    requestBody.max_tokens = 16384; // ‚úÖ TIER 2: M√°ximo output para chunks Map-Reduce
    requestBody.temperature = 0.7;
  } else {
    requestBody.max_completion_tokens = 16384; // ‚úÖ TIER 2: M√°ximo output para chunks Map-Reduce
  }
  
  console.log(`üìù Output config for chunk ${chunkIndex + 1}: max_completion_tokens=${requestBody.max_completion_tokens || requestBody.max_tokens}`);

  for (let attempt = 0; attempt <= retries; attempt++) {
    try {
      const response = await fetch("https://api.openai.com/v1/chat/completions", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${openAIApiKey}`,
          "Content-Type": "application/json",
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        if (response.status === 429 && attempt < retries) {
          const delay = Math.pow(2, attempt) * 1000;
          console.log(`‚è±Ô∏è Rate limit hit, waiting ${delay}ms...`);
          await new Promise(resolve => setTimeout(resolve, delay));
          continue;
        }
        throw new Error(`OpenAI API error: ${response.status}`);
      }

      const data = await response.json();
      const content = data.choices[0].message.content;
      console.log(`‚úÖ Chunk ${chunkIndex + 1}/${totalChunks} processed (${estimateTokens(content)} tokens)`);
      return content;
    } catch (error) {
      if (attempt === retries) throw error;
      console.log(`‚ö†Ô∏è Retry ${attempt + 1}/${retries} for chunk ${chunkIndex + 1}`);
    }
  }
  
  throw new Error(`Failed to process chunk ${chunkIndex + 1} after ${retries} retries`);
};

// Fun√ß√£o para consolidar respostas e fazer streaming
const consolidateResponses = async (
  chunkResponses: string[],
  originalMessage: string,
  model: string,
  openAIApiKey: string
): Promise<ReadableStream> => {
  console.log(`üß† Consolidating ${chunkResponses.length} chunk responses...`);
  
  const consolidationPrompt = `Voc√™ analisou um documento longo dividido em ${chunkResponses.length} partes. Aqui est√£o as an√°lises de cada parte:

${chunkResponses.map((resp, i) => `\n[PARTE ${i + 1}/${chunkResponses.length}]\n${resp}\n`).join('\n---\n')}

Agora, crie uma resposta FINAL consolidada e profunda (12.000-16.000 palavras, aproximadamente 16 p√°ginas) que:
- Resume os pontos mais importantes de TODAS as partes
- Identifica padr√µes e conex√µes entre as diferentes se√ß√µes
- Fornece insights profundos e an√°lise cr√≠tica
- Organiza a informa√ß√£o de forma l√≥gica e bem estruturada
- Inclui exemplos e cita√ß√µes espec√≠ficas do documento
- Mantenha o formato em Markdown com t√≠tulos, subt√≠tulos e listas

Pergunta/contexto original do usu√°rio: ${originalMessage}`;

  const isNewerModel = model.includes("gpt-5") || model.includes("gpt-4.1") || model.includes("o3") || model.includes("o4");
  
  const requestBody: any = {
    model,
    messages: [
      {
        role: "system",
        content: "Voc√™ √© um assistente especializado em criar an√°lises consolidadas extremamente detalhadas e profundas de documentos. Suas respostas devem ser extensas, bem estruturadas e ricas em insights."
      },
      {
        role: "user",
        content: consolidationPrompt
      }
    ],
    stream: true,
  };

  if (!isNewerModel) {
    requestBody.max_tokens = 16384;
    requestBody.temperature = 0.7;
  } else {
    requestBody.max_completion_tokens = 16384;
  }

  const response = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      "Authorization": `Bearer ${openAIApiKey}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify(requestBody),
  });

  if (!response.ok) {
    throw new Error(`Consolidation failed: ${response.status}`);
  }

  return response.body!;
};

serve(async (req) => {
  if (req.method === "OPTIONS") {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { message, model = "gpt-5-mini-2025-08-07", files = [], conversationHistory = [], hasLargeDocument = false } = await req.json();

    // ‚úÖ Mapear nome do modelo para o formato da API OpenAI
    const apiModel = mapModelName(model);
    console.log(`üìã Using model: ${apiModel}${model !== apiModel ? ` (original: ${model})` : ''}`);

    const openAIApiKey = Deno.env.get("OPENAI_API_KEY");
    if (!openAIApiKey) {
      throw new Error("OPENAI_API_KEY n√£o configurada");
    }
    
    // ‚úÖ Log para confirmar qual chave est√° sendo usada (primeiros 10 + √∫ltimos 4 chars)
    console.log(`üîë Using API Key: ${openAIApiKey.substring(0, 10)}...${openAIApiKey.substring(openAIApiKey.length - 4)}`);

    // Estimar tokens da mensagem
    const estimatedTokens = estimateTokens(message);
    console.log(`üìä Token estimation: ${estimatedTokens} tokens for model ${apiModel}`);
    console.log(`üîç Document size: ${estimatedTokens} tokens (${Math.ceil(estimatedTokens / 400)} p√°ginas aprox.)`);

    // ‚úÖ TIER-2-MAXOUT-PLUS: Threshold din√¢mico baseado no modelo (usa nome original)
    const threshold = getMapReduceThreshold(model);
    const needsMapReduce = hasLargeDocument && estimatedTokens > threshold;
    
    console.log(`üìä Map-Reduce Decision:`);
    console.log(`  - Estimated tokens: ${estimatedTokens}`);
    console.log(`  - Threshold: ${threshold}`);
    console.log(`  - Has large document: ${hasLargeDocument}`);
    console.log(`  - Result: ${needsMapReduce ? 'ATIVADO ‚úÖ' : 'DESATIVADO ‚ùå'}`);

    if (needsMapReduce) {
      console.log(`üóÇÔ∏è Large document detected (${estimatedTokens} tokens) - using Map-Reduce approach`);
      
      try {
        // MAP PHASE: Dividir em chunks e processar cada um
        const chunks = chunkText(message, 20000); // ‚úÖ TIER 2: ~20k tokens por chunk (~50 p√°ginas)
        console.log(`üìö Processing ${chunks.length} chunks in parallel...`);
        
        const chunkPromises = chunks.map((chunk, i) => 
          processChunk(chunk, i, chunks.length, apiModel, openAIApiKey, 1)
        );
        
        const chunkResponses = await Promise.all(chunkPromises);
        console.log(`‚úÖ All ${chunks.length} chunks processed successfully`);
        
        // REDUCE PHASE: Consolidar respostas e fazer streaming
        const consolidatedStream = await consolidateResponses(
          chunkResponses,
          conversationHistory.length > 0 
            ? conversationHistory[conversationHistory.length - 1].content 
            : "Analise este documento",
          apiModel,
          openAIApiKey
        );
        
        console.log("üß† Streaming consolidated response...");
        
        return new Response(consolidatedStream, {
          headers: {
            ...corsHeaders,
            "Content-Type": "text/event-stream",
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
          },
        });
      } catch (error: any) {
        console.error("‚ùå Map-Reduce error:", error);
        return new Response(
          JSON.stringify({ error: `Map-Reduce processing failed: ${error.message}` }),
          {
            status: 500,
            headers: { ...corsHeaders, "Content-Type": "application/json" },
          }
        );
      }
    }

    // STREAMING DIRETO para documentos menores
    console.log("üìù Document size OK - using direct streaming");
    
    // Preparar mensagens para OpenAI
    const messages: any[] = [
      {
        role: "system",
        content: "Voc√™ √© um assistente √∫til e preciso. Responda de forma clara e organizada.",
      },
    ];

    // Adicionar hist√≥rico de conversa
    if (conversationHistory && conversationHistory.length > 0) {
      conversationHistory.forEach((msg: any) => {
        messages.push({
          role: msg.role,
          content: msg.content,
        });
      });
    }

    // Processar arquivos (imagens para vis√£o)
    if (files && files.length > 0) {
      const imageFiles = files.filter((f: any) => f.type?.startsWith("image/"));
      
      if (imageFiles.length > 0) {
        // Modelo com vis√£o - formato especial
        const content: any[] = [{ type: "text", text: message }];
        
        imageFiles.forEach((file: any) => {
          if (file.imageData) {
            content.push({
              type: "image_url",
              image_url: {
                url: file.imageData,
              },
            });
          }
        });

        messages.push({
          role: "user",
          content,
        });
      } else {
        // Sem imagens - mensagem de texto simples
        messages.push({
          role: "user",
          content: message,
        });
      }
    } else {
      messages.push({
        role: "user",
        content: message,
      });
    }

    console.log(`üöÄ Sending request to OpenAI with model: ${apiModel}`);

    // Determinar par√¢metros baseado no modelo
    const isNewerModel = apiModel.includes("gpt-5") || apiModel.includes("gpt-4.1") || apiModel.includes("o3") || apiModel.includes("o4");
    
    const requestBody: any = {
      model: apiModel,
      messages,
      stream: true,
    };

    // ‚úÖ TIER-2-MAXOUT-PLUS: Output din√¢mico baseado no tamanho do input
    const maxOutputTokens = Math.min(
      16384, // M√°ximo absoluto (Tier 2)
      Math.max(
        8000,  // M√≠nimo garantido
        16384 - Math.floor(estimatedTokens * 1.2) // Margem de seguran√ßa para evitar overflow
      )
    );
    
    console.log(`üí° Dynamic output: ${maxOutputTokens} tokens (input: ${estimatedTokens} tokens, ratio: ${(maxOutputTokens/estimatedTokens).toFixed(1)}x)`);

    // Apenas modelos antigos suportam max_tokens e temperature
    if (!isNewerModel) {
      requestBody.max_tokens = maxOutputTokens;
      requestBody.temperature = 0.7;
    } else {
      // Modelos novos usam max_completion_tokens
      requestBody.max_completion_tokens = maxOutputTokens;
    }
    
    console.log(`üìù Direct streaming output config: max_completion_tokens=${requestBody.max_completion_tokens || requestBody.max_tokens}`);

    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${openAIApiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify(requestBody),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error("‚ùå OpenAI API error:", response.status, errorText);
      
      if (response.status === 429) {
        return new Response(
          JSON.stringify({ error: "Rate limit exceeded. Please try again later." }),
          {
            status: 429,
            headers: { ...corsHeaders, "Content-Type": "application/json" },
          }
        );
      }
      
      if (response.status === 402) {
        return new Response(
          JSON.stringify({ error: "Insufficient credits. Please add funds to your OpenAI account." }),
          {
            status: 402,
            headers: { ...corsHeaders, "Content-Type": "application/json" },
          }
        );
      }

      throw new Error(`OpenAI API error: ${response.status} - ${errorText}`);
    }

    console.log("‚úÖ Streaming response from OpenAI");

    // Retornar stream SSE diretamente
    return new Response(response.body, {
      headers: {
        ...corsHeaders,
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
      },
    });

  } catch (error: any) {
    console.error("‚ùå Error in openai-chat:", error);
    return new Response(
      JSON.stringify({ error: error.message || "Unknown error occurred" }),
      {
        status: 500,
        headers: { ...corsHeaders, "Content-Type": "application/json" },
      }
    );
  }
});
